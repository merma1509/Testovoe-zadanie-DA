# Задача 1: Пони тоже кони

## Условие

Вас просят разработать модель, классифицирующую лошадок и пони. Вместо разработки вы нашли на GitHub две интересные модели и после прогона на ваших данных одна из них показала ROC-AUC=0.7, а другая ROC-AUC=0.1. Какую модель вы возьмете для дальнейшей работы и что будете с ней делать?

## Анализ

### Понимание ROC-AUC

**ROC-AUC** — площадь под ROC-кривой:

- **0.5**: случайное угадывание
- **1.0**: идеальная классификация
- **0.0**: идеально неправильная классификация

### Анализ моделей

#### Модель 1: ROC-AUC = 0.7

- **Качество**: выше случайного угадывания
- **Интерпретация**: модель правильно ранжирует 70% пар объектов
- **Статус**: работающая, но не идеальная модель

#### Модель 2: ROC-AUC = 0.1

- **Качество**: хуже случайного угадывания
- **Ключевое наблюдение**: ROC-AUC = 0.1 означает систематическую ошибку
- **Инсайт**: модель "научилась" делать наоборот!

### Стратегическое решение

#### Почему модель с ROC-AUC = 0.1 ценна?

**Инвертированная модель**: Если модель систематически ошибается, то инвертировав ее предсказания, мы получим модель с ROC-AUC = 0.9!

**Математически**:
$$\text{Новый ROC-AUC} = 1 - \text{Старый ROC-AUC} = 1 - 0.1 = 0.9$$

## Ответ:**Возьму вторую модель с ROC-AUC=0.1, инвертирую ее предсказания и получу модель с ROC-AUC=0.9.**

## План действий

### Шаг 1: Инвертирование предсказаний

```python
# Если модель выдает вероятности класса 1
inverted_predictions = 1 - original_predictions
```

### Шаг 2: Валидация инвертированной модели

- Пересчитать ROC-AUC на валидационной выборке
- Проверить другие метрики (precision, recall, F1)
- Убедиться в стабильности результата

### Шаг 3: Сравнение с первой моделью

- **Модель 1**: ROC-AUC = 0.7
- **Модель 2 (инвертированная)**: ROC-AUC = 0.9
- **Победитель**: очевидно, вторая модель

### Шаг 4: Дальнейшая работа

1. **Анализ ошибок**: почему исходная модель работала "наоборот"?
2. **Дообучение**: возможно, проблема в метках классов
3. **Ансамбль**: комбинировать с первой моделью
4. **Продакшн**: развернуть инвертированную модель

## Глубокий анализ

### Почему модель могла показать ROC-AUC = 0.1?

#### Возможные причины:

1. **Перепутанные метки**: класс 0 и 1 поменяны местами
2. **Обратная задача**: модель решала противоположную задачу
3. **Баг в коде**: инвертирование предсказаний где-то в пайплайне
4. **Особенность данных**: специфика доменной области

#### Диагностика:

```python
# Проверить распределение предсказаний для каждого класса
import matplotlib.pyplot as plt

plt.hist(predictions[true_labels == 0], alpha=0.5, label='Class 0')
plt.hist(predictions[true_labels == 1], alpha=0.5, label='Class 1')
plt.legend()
plt.show()
```

### Риски инвертирования

1. **Стабильность**: будет ли модель работать на новых данных?
2. **Интерпретируемость**: сложнее объяснить "инвертированную" логику
3. **Отладка**: проблемы с отладкой и мониторингом

### Митигация рисков

- Тщательная валидация на нескольких выборках
- A/B тестирование в продакшене
- Мониторинг качества в реальном времени

### Практические соображения

#### Преимущества подхода:

1. **Быстрый результат**: не нужно обучать с нуля
2. **Высокое качество**: ROC-AUC 0.9 > 0.7
3. **Экономия ресурсов**: переиспользование существующей модели

#### Когда бы выбрал первую модель:

- Если бы ROC-AUC второй был ~0.5 (случайное качество)
- Если бы были сомнения в стабильности инвертированных предсказаний
- Если бы требовалась высокая интерпретируемость

#### Итоговое решение

**Выбираю вторую модель, инвертирую предсказания, получаю ROC-AUC = 0.9. Это классический пример "плохой" модели, которая на самом деле "хорошая", но работает в обратную сторону.**
